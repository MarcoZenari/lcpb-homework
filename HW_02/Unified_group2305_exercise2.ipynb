{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ece727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import pandas as pd\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "from metrics import pair_error, sequence_error, JS_divergence, p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd4f1e",
   "metadata": {},
   "source": [
    "## Boltzmann machine class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class boltzmanmachine:\n",
    "    \n",
    "    def __init__(self, L, M, sigma, De=2, vmin=-1, spin=True):\n",
    "        '''\n",
    "        Initialize the object with defaults parameter which are determined by\n",
    "        meta reciew.\n",
    "        w: network weights\n",
    "        a: visible bias\n",
    "        b: hidden bias\n",
    "        spin: set the notation used by the model, (0,1) or (-1,1)\n",
    "        DE: energy separation among the two possible states allowed in the positive phases, 1 or 2\n",
    "        vmin: minimum value of the representation\n",
    "        '''\n",
    "        self.w = np.random.normal(loc=0.0, scale=sigma, size=(L,M))\n",
    "        self.a = np.random.normal(loc=0.0, scale=sigma, size=L)\n",
    "        self.b = b = np.zeros(M)\n",
    "        \n",
    "        self.spin=spin\n",
    "        \n",
    "        self.DE = De\n",
    "        self.L = int(L)\n",
    "        self.M = int(M)\n",
    "        self.vmin=vmin\n",
    "        \n",
    "        self.v_data, self.v_model = None, None\n",
    "        self.h_data, self.h_model = None, None\n",
    "        self.vh_data,self.vh_model= None, None\n",
    "        \n",
    "        #one hot encoding of the four possible states\n",
    "        if self.spin:\n",
    "            self.csi1 = np.array([1,-1,-1,-1])    \n",
    "            self.csi2 = np.array([-1,1,-1,-1])   \n",
    "            self.csi3 = np.array([-1,-1,1,-1])    \n",
    "            self.csi4 = np.array([-1,-1,-1,1])  \n",
    "        else:\n",
    "            self.csi1 = np.array([1,0,0,0])    \n",
    "            self.csi2 = np.array([0,1,0,0])   \n",
    "            self.csi3 = np.array([0,0,1,0])    \n",
    "            self.csi4 = np.array([0,0,0,1]) \n",
    "            \n",
    "        self.csi = [self.csi1, self.csi2, self.csi3, self.csi4]\n",
    "        \n",
    "        #algotithm variables\n",
    "        self.sa_t0 = 0\n",
    "        self.sb_t0 = 0\n",
    "        self.sw_t0 = 0\n",
    "        \n",
    "        self.ma_t0 = 0\n",
    "        self.mb_t0 = 0\n",
    "        self.mw_t0 = 0\n",
    "        \n",
    "        self.batch_counter = 0\n",
    "        \n",
    "    def load_model(self, file_name):\n",
    "        model = np.load(file_name)\n",
    "        \n",
    "        self.w = model['w']\n",
    "        self.a = model['a']\n",
    "        self.b = model['b']\n",
    "        \n",
    "        \n",
    "        \n",
    "    def save_model(self, file_name):\n",
    "        np.savez(file_name, a=self.a, b=self.b, w=self.w)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_avg(self):\n",
    "        '''\n",
    "        Set  to zero the averages quantities needed to compute the gradien\n",
    "        '''\n",
    "        self.v_data, self.v_model = np.zeros(self.L),np.zeros(self.L)\n",
    "        self.h_data, self.h_model = np.zeros(self.M),np.zeros(self.M)\n",
    "        self.vh_data,self.vh_model= np.zeros((self.L,self.M)),np.zeros((self.L,self.M))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def positive(self, v_in, Amp=1.):\n",
    "        '''\n",
    "        Positive phase of the training\n",
    "        Visible -> Hidden\n",
    "        No one-hot encoding needed\n",
    "        '''\n",
    "        act = np.dot(v_in, self.w) + self.b      \n",
    "        #print(act)\n",
    "        argument=np.exp(-Amp*self.DE*act)\n",
    "        prob = 1. / (1. + argument)\n",
    "        n = np.shape(act)\n",
    "        h = np.full(n, self.vmin, dtype=int) # a list on -1's or 0's\n",
    "        h[np.random.random_sample(n) < prob] = 1\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def neg(self, h_in, Amp=1.):\n",
    "        '''\n",
    "        Negative phase of the training\n",
    "        Hidden -> Visible\n",
    "        No one-hot encoding needed\n",
    "        '''\n",
    "        act = np.dot(h_in, self.w.T) + self.a      \n",
    "        #print(act)\n",
    "        prob = 1. / (1. + np.exp(-Amp*self.DE*act))\n",
    "        n = np.shape(act)\n",
    "        vf = np.full(n, self.vmin, dtype=int) # a list on -1's or 0's\n",
    "        vf[np.random.random_sample(n) < prob] = 1\n",
    "        \n",
    "        return vf\n",
    "    \n",
    "    \n",
    "    def negative(self, h_in, Amp=1.):\n",
    "        '''\n",
    "        Negative phase of the training\n",
    "        Hidden -> Visible\n",
    "        With ne-hot encoding needed\n",
    "        '''\n",
    "        \n",
    "        weigths = np.reshape(np.dot(h_in, self.w.T) + self.a, (5,4) ) \n",
    "\n",
    "        \n",
    "        E1 = np.dot(weigths, self.csi1) #array of length 5, the number of amminoacids\n",
    "        E2 = np.dot(weigths, self.csi2)\n",
    "        E3 = np.dot(weigths, self.csi3)\n",
    "        E4 = np.dot(weigths, self.csi4)\n",
    "        \n",
    "\n",
    "\n",
    "        Z = np.exp(-Amp*E1) + np.exp(-Amp*E2) + np.exp(-Amp*E3) + np.exp(-Amp*E4) #partition function for each amminoacid\n",
    "        \n",
    "        p1 = np.exp(-Amp*E1)/Z \n",
    "        p2 = np.exp(-Amp*E2)/Z\n",
    "        p3 = np.exp(-Amp*E3)/Z\n",
    "        p4 = np.exp(-Amp*E4)/Z\n",
    "        \n",
    "        \n",
    "        \n",
    "        p = np.reshape(np.concatenate((p1, p2, p3, p4)), (4, 5))      \n",
    "\n",
    "        \n",
    "        cum = np.cumsum(p, axis=0) #(4x5) containing the comulatives  \n",
    "        r = np.random.random(size=5)        \n",
    "        \n",
    "        mask = cum < r    \n",
    "        indx = []    \n",
    "        \n",
    "        for i in range(mask.shape[1]):\n",
    "            __, index = np.unique(mask[:, i], return_index=True)\n",
    "            indx.append(index[0])\n",
    "            \n",
    "        vf=np.concatenate((self.csi[indx[0]], self.csi[indx[1]], self.csi[indx[2]], self.csi[indx[3]], self.csi[indx[4]]))\n",
    "        return vf\n",
    "        \n",
    "    \n",
    "    def update_vh(self, v_k, vf, h, hf, mini):\n",
    "        '''\n",
    "        Update the averages needed to compute the gradient\n",
    "        '''\n",
    "        self.v_data  += v_k/mini\n",
    "        self.v_model += vf/mini\n",
    "        self.h_data  += h/mini\n",
    "        self.h_model += hf/mini\n",
    "        self.vh_data += np.outer(v_k.T,h)/mini\n",
    "        self.vh_model+= np.outer(vf.T,hf)/mini\n",
    "    \n",
    "    def SGD(self, l_rate_m):\n",
    "        '''\n",
    "        Stochastic gradient descent algorithm\n",
    "        '''\n",
    "        dw = l_rate_m*(self.vh_data - self.vh_model)\n",
    "        da = l_rate_m*(self.v_data - self.v_model)\n",
    "        db = l_rate_m*(self.h_data - self.h_model)\n",
    "        \n",
    "        self.w += dw\n",
    "        self.a += da\n",
    "        self.b += db\n",
    "        \n",
    "    def RMSprop(self, eta_t, beta=0.9, epsilon=1e-8):\n",
    "        '''\n",
    "        RMSprop algorithm\n",
    "        '''\n",
    "        ga_t = self.v_data - self.v_model\n",
    "        gb_t = self.h_data - self.h_model\n",
    "        gw_t = self.vh_data - self.vh_model\n",
    "        \n",
    "        sa_t = beta*self.sa_t0 + (1-beta)*ga_t**2\n",
    "        sb_t = beta*self.sb_t0 + (1-beta)*gb_t**2\n",
    "        sw_t = beta*self.sw_t0 + (1-beta)*gw_t**2\n",
    "        \n",
    "        self.sa_t0 = sa_t\n",
    "        self.sb_t0 = sb_t\n",
    "        self.sw_t0 = sw_t\n",
    "        \n",
    "        \n",
    "        self.a = self.a + eta_t*ga_t/np.sqrt(sa_t + epsilon)\n",
    "        self.b = self.b + eta_t*gb_t/np.sqrt(sb_t + epsilon)\n",
    "        self.w = self.w + eta_t*gw_t/np.sqrt(sw_t + epsilon)\n",
    "        \n",
    "        \n",
    "    def ADAM(self, eta_t, epoch, beta1=0.9, beta2=0.99,epsilon=1e-8):\n",
    "        '''\n",
    "        ADAM algorithm\n",
    "        '''\n",
    "        ga_t = self.v_data - self.v_model\n",
    "        gb_t = self.h_data - self.h_model\n",
    "        gw_t = self.vh_data - self.vh_model\n",
    "\n",
    "        ma_t = beta1*self.ma_t0 + (1-beta1)*ga_t\n",
    "        mb_t = beta1*self.mb_t0 + (1-beta1)*gb_t\n",
    "        mw_t = beta1*self.mw_t0 + (1-beta1)*gw_t\n",
    "\n",
    "        sa_t = beta2*self.sa_t0 + (1-beta2)*ga_t**2\n",
    "        sb_t = beta2*self.sb_t0 + (1-beta2)*gb_t**2\n",
    "        sw_t = beta2*self.sw_t0 + (1-beta2)*gw_t**2\n",
    "\n",
    "        self.sa_t0 = sa_t\n",
    "        self.sb_t0 = sb_t\n",
    "        self.sw_t0 = sw_t\n",
    "\n",
    "        self.ma_t0 = ma_t\n",
    "        self.mb_t0 = mb_t\n",
    "        self.mw_t0 = mw_t\n",
    "\n",
    "        ma_t_hat = ma_t/(1-beta1**epoch) \n",
    "        mb_t_hat = mb_t/(1-beta1**epoch) \n",
    "        mw_t_hat = mw_t/(1-beta1**epoch)\n",
    "\n",
    "        sa_t_hat = sa_t/(1-beta2**epoch) \n",
    "        sb_t_hat = sb_t/(1-beta2**epoch) \n",
    "        sw_t_hat = sw_t/(1-beta2**epoch)\n",
    "\n",
    "        self.a = self.a + eta_t*ma_t_hat/(np.sqrt(sa_t_hat) + epsilon)\n",
    "        self.b = self.b + eta_t*mb_t_hat/(np.sqrt(sb_t_hat) + epsilon)\n",
    "        self.w = self.w + eta_t*mw_t_hat/(np.sqrt(sw_t_hat) + epsilon)\n",
    "        \n",
    "    def train(self, data, learning_rate, batch_size, n_contrastive_div, Amp_training, Algorithm, epoch=1):\n",
    "        \n",
    "        if self.batch_counter == 0:\n",
    "            self.init_avg()\n",
    "            \n",
    "        v_k = np.copy(data)\n",
    "        vf = np.copy(data)\n",
    "        \n",
    "        for i in np.arange(n_contrastive_div):\n",
    "            h = self.positive(vf, Amp_training)\n",
    "            vf = self.negative(h, Amp_training)\n",
    "        hf = self.positive(vf, Amp_training)\n",
    "        \n",
    "        self.update_vh(v_k, vf, h, hf, batch_size)\n",
    "        \n",
    "        self.batch_counter += 1\n",
    "        \n",
    "        if self.batch_counter == batch_size:\n",
    "            if Algorithm == 'SGD':\n",
    "                self.SGD(learning_rate)\n",
    "            if Algorithm == 'RMSprop':\n",
    "                self.RMSprop(learning_rate)\n",
    "            if Algorithm == 'Adam':\n",
    "                self.ADAM(l_rate, epoch+1)\n",
    "                \n",
    "            self.batch_counter = 0\n",
    "\n",
    "\n",
    "    def gen_fantasy(self, data, Amp_gen):\n",
    "        \n",
    "        vf = np.zeros_like(data)\n",
    "        N = data.shape[0]\n",
    "        \n",
    "        for k in range(N):\n",
    "            # positive CD phase: generating h \n",
    "            h = self.positive(data[k],Amp_gen)\n",
    "            # negative CD phase: generating fantasy vf with low T == large GAP\n",
    "            vf[k] = self.negative(h,Amp_gen)\n",
    "            \n",
    "        return vf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
